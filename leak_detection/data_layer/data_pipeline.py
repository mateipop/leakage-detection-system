"""
Data Pipeline - Validation, filtering, and normalization of telemetry data.
"""

import logging
import math
from typing import Optional, Callable, List, Dict
from dataclasses import dataclass
from collections import deque

import numpy as np

from ..config import DataLayerConfig, DEFAULT_CONFIG
from ..simulation.device_simulator import SensorReading
from .telemetry_buffer import TelemetryBuffer, TelemetryRecord

logger = logging.getLogger(__name__)


@dataclass
class PipelineEvent:
    """Event generated by the pipeline for logging/monitoring."""
    event_type: str  # 'validation_error', 'filter_applied', 'anomaly_flagged'
    node_id: str
    timestamp: float
    message: str
    severity: str = 'info'  # 'info', 'warning', 'error'


class DataPipeline:
    """
    Data processing pipeline for sensor telemetry.

    Pipeline stages:
    1. Validation - Check for NaN/corrupted/out-of-range values
    2. Storage - Save to telemetry buffer
    3. Filtering - Apply moving average noise filter
    4. Normalization - Calculate Z-score
    """

    def __init__(
        self,
        config: DataLayerConfig = None,
        event_callback: Callable[[PipelineEvent], None] = None
    ):
        """
        Initialize the data pipeline.

        Args:
            config: Data layer configuration
            event_callback: Function called when pipeline events occur
        """
        self.config = config or DEFAULT_CONFIG.data_layer
        self._event_callback = event_callback

        # Storage
        self._buffer = TelemetryBuffer(self.config)

        # Moving average filter windows per node
        self._filter_windows: Dict[str, Dict[str, deque]] = {}

        # Statistics
        self._processed_count = 0
        self._valid_count = 0
        self._invalid_count = 0

        logger.info("DataPipeline initialized")

    def _emit_event(
        self,
        event_type: str,
        node_id: str,
        timestamp: float,
        message: str,
        severity: str = 'info'
    ):
        """Emit a pipeline event."""
        event = PipelineEvent(
            event_type=event_type,
            node_id=node_id,
            timestamp=timestamp,
            message=message,
            severity=severity
        )
        logger.log(
            logging.WARNING if severity == 'error' else logging.DEBUG,
            f"Pipeline [{event_type}] {node_id}: {message}"
        )
        if self._event_callback:
            self._event_callback(event)

    def _validate(self, reading: SensorReading) -> tuple[bool, Optional[str]]:
        """
        Validate a sensor reading.

        Returns:
            Tuple of (is_valid, error_message)
        """
        # Check for NaN
        if reading.value is None or math.isnan(reading.value):
            return False, "NaN value detected"

        # Check for infinity
        if math.isinf(reading.value):
            return False, "Infinite value detected"

        # Range checks based on reading type
        if reading.reading_type == 'pressure':
            if reading.value < self.config.validation_min_pressure:
                return False, f"Pressure below minimum ({reading.value:.2f} < {self.config.validation_min_pressure})"
            if reading.value > self.config.validation_max_pressure:
                return False, f"Pressure above maximum ({reading.value:.2f} > {self.config.validation_max_pressure})"
        elif reading.reading_type == 'flow':
            if reading.value < self.config.validation_min_flow:
                return False, f"Flow below minimum ({reading.value:.2f} < {self.config.validation_min_flow})"
            if reading.value > self.config.validation_max_flow:
                return False, f"Flow above maximum ({reading.value:.2f} > {self.config.validation_max_flow})"

        # Check if device flagged as corrupted
        if reading.is_corrupted:
            return False, "Device flagged reading as corrupted"

        return True, None

    def _apply_moving_average(
        self,
        node_id: str,
        reading_type: str,
        value: float
    ) -> float:
        """
        Apply moving average filter.

        Returns:
            Filtered value
        """
        # Ensure window exists
        if node_id not in self._filter_windows:
            self._filter_windows[node_id] = {}
        if reading_type not in self._filter_windows[node_id]:
            self._filter_windows[node_id][reading_type] = deque(
                maxlen=self.config.moving_average_window
            )

        window = self._filter_windows[node_id][reading_type]
        window.append(value)

        # Return average
        return sum(window) / len(window)

    def _calculate_zscore(
        self,
        node_id: str,
        reading_type: str,
        value: float
    ) -> Optional[float]:
        """
        Calculate Z-score based on historical statistics.

        Returns:
            Z-score or None if insufficient history
        """
        stats = self._buffer.get_windowed_statistics(
            node_id,
            reading_type,
            self.config.zscore_window
        )

        if stats is None or stats['count'] < self.config.zscore_window // 2:
            return None

        if stats['std'] < 1e-6:  # Avoid division by zero
            return 0.0

        z_score = (value - stats['mean']) / stats['std']
        return z_score

    def process(self, reading: SensorReading) -> Optional[TelemetryRecord]:
        """
        Process a sensor reading through the full pipeline.

        Pipeline: Validate -> Store -> Filter -> Normalize

        Args:
            reading: Raw sensor reading

        Returns:
            Processed TelemetryRecord or None if invalid
        """
        self._processed_count += 1

        # Stage 1: Validation
        is_valid, error_msg = self._validate(reading)

        record = TelemetryRecord(
            node_id=reading.node_id,
            timestamp=reading.timestamp,
            reading_type=reading.reading_type,
            raw_value=reading.value if is_valid else None,
            is_valid=is_valid,
            validation_error=error_msg
        )

        if not is_valid:
            self._invalid_count += 1
            self._emit_event(
                'validation_error',
                reading.node_id,
                reading.timestamp,
                f"Invalid data discarded: {error_msg}",
                'warning'
            )
            # Still store the record for tracking purposes
            self._buffer.store(record)
            return None

        self._valid_count += 1

        # Stage 2: Store raw value
        # (We'll update the record with filtered/normalized values)

        # Stage 3: Apply moving average filter
        filtered_value = self._apply_moving_average(
            reading.node_id,
            reading.reading_type,
            reading.value
        )
        record.filtered_value = filtered_value

        # Stage 4: Calculate Z-score normalization
        z_score = self._calculate_zscore(
            reading.node_id,
            reading.reading_type,
            filtered_value
        )
        record.z_score = z_score

        # Store the complete record
        self._buffer.store(record)

        self._emit_event(
            'data_processed',
            reading.node_id,
            reading.timestamp,
            f"{reading.reading_type}: raw={reading.value:.2f}, filtered={filtered_value:.2f}, z={z_score:.2f}" if z_score else f"{reading.reading_type}: raw={reading.value:.2f}, filtered={filtered_value:.2f}",
            'info'
        )

        return record

    def process_batch(
        self,
        readings: List[SensorReading]
    ) -> List[TelemetryRecord]:
        """
        Process multiple readings.

        Args:
            readings: List of sensor readings

        Returns:
            List of valid processed records
        """
        results = []
        for reading in readings:
            record = self.process(reading)
            if record is not None:
                results.append(record)
        return results

    def get_latest_record(
        self,
        node_id: str,
        reading_type: str
    ) -> Optional[TelemetryRecord]:
        """Get the most recent valid record for a node."""
        records = self._buffer.get_recent(node_id, reading_type, 1)
        return records[0] if records else None

    def get_node_statistics(
        self,
        node_id: str,
        reading_type: str
    ) -> Optional[Dict]:
        """Get statistics for a node."""
        return self._buffer.get_windowed_statistics(node_id, reading_type)

    def get_all_latest_zscores(self) -> Dict[str, Dict[str, float]]:
        """
        Get latest Z-scores for all nodes.

        Returns:
            Dict[node_id, Dict[reading_type, z_score]]
        """
        result = {}
        for node_id in self._buffer.get_all_nodes():
            result[node_id] = {}
            for reading_type in self._buffer.get_node_types(node_id):
                records = self._buffer.get_recent(node_id, reading_type, 1)
                if records and records[0].z_score is not None:
                    result[node_id][reading_type] = records[0].z_score
        return result

    @property
    def buffer(self) -> TelemetryBuffer:
        """Access to the underlying buffer."""
        return self._buffer

    @property
    def statistics(self) -> Dict:
        """Get pipeline processing statistics."""
        return {
            'processed': self._processed_count,
            'valid': self._valid_count,
            'invalid': self._invalid_count,
            'validity_rate': (
                self._valid_count / self._processed_count
                if self._processed_count > 0 else 0.0
            )
        }

    def reset(self):
        """Reset the pipeline state."""
        self._buffer.clear()
        self._filter_windows.clear()
        self._processed_count = 0
        self._valid_count = 0
        self._invalid_count = 0
