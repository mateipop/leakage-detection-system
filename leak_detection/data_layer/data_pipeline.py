"""
Data Pipeline - Validation, filtering, and normalization of telemetry data.
"""

import logging
import math
from typing import Optional, Callable, List, Dict
from dataclasses import dataclass
from collections import deque

import numpy as np

from ..config import DataLayerConfig, DEFAULT_CONFIG
from ..simulation.device_simulator import SensorReading
from .telemetry_buffer import TelemetryBuffer, TelemetryRecord

logger = logging.getLogger(__name__)


@dataclass
class PipelineEvent:
    """Event generated by the pipeline for logging/monitoring."""
    event_type: str  # 'validation_error', 'filter_applied', 'anomaly_flagged'
    node_id: str
    timestamp: float
    message: str
    severity: str = 'info'  # 'info', 'warning', 'error'


class DataPipeline:
    """
    Data processing pipeline for sensor telemetry.

    Pipeline stages:
    1. Validation - Check for NaN/corrupted/out-of-range values
    2. Storage - Save to telemetry buffer
    3. Filtering - Apply moving average noise filter
    4. Normalization - Calculate Z-score
    """

    def __init__(
        self,
        config: DataLayerConfig = None,
        event_callback: Callable[[PipelineEvent], None] = None
    ):
        """
        Initialize the data pipeline.

        Args:
            config: Data layer configuration
            event_callback: Function called when pipeline events occur
        """
        self.config = config or DEFAULT_CONFIG.data_layer
        self._event_callback = event_callback

        # Storage
        self._buffer = TelemetryBuffer(self.config)

        # Moving average filter windows per node
        self._filter_windows: Dict[str, Dict[str, deque]] = {}

        # Statistics
        self._processed_count = 0
        self._valid_count = 0
        self._invalid_count = 0
        
        # Baseline snapshot - frozen stats from initial period
        # Key: (node_id, reading_type) -> {'mean': float, 'std': float}
        self._baseline_snapshot: Dict[tuple, Dict[str, float]] = {}
        self._baseline_locked = False
        self._baseline_samples_needed = 20  # Samples before locking baseline

        logger.info("DataPipeline initialized")

    def _emit_event(
        self,
        event_type: str,
        node_id: str,
        timestamp: float,
        message: str,
        severity: str = 'info'
    ):
        """Emit a pipeline event."""
        event = PipelineEvent(
            event_type=event_type,
            node_id=node_id,
            timestamp=timestamp,
            message=message,
            severity=severity
        )
        logger.log(
            logging.WARNING if severity == 'error' else logging.DEBUG,
            f"Pipeline [{event_type}] {node_id}: {message}"
        )
        if self._event_callback:
            self._event_callback(event)

    def _validate(self, reading: SensorReading) -> tuple[bool, Optional[str]]:
        """
        Validate a sensor reading.

        Returns:
            Tuple of (is_valid, error_message)
        """
        # Check for NaN
        if reading.value is None or math.isnan(reading.value):
            return False, "NaN value detected"

        # Check for infinity
        if math.isinf(reading.value):
            return False, "Infinite value detected"

        # Range checks based on reading type
        if reading.reading_type == 'pressure':
            if reading.value < self.config.validation_min_pressure:
                return False, f"Pressure below minimum ({reading.value:.2f} < {self.config.validation_min_pressure})"
            if reading.value > self.config.validation_max_pressure:
                return False, f"Pressure above maximum ({reading.value:.2f} > {self.config.validation_max_pressure})"
        elif reading.reading_type == 'flow':
            if reading.value < self.config.validation_min_flow:
                return False, f"Flow below minimum ({reading.value:.2f} < {self.config.validation_min_flow})"
            if reading.value > self.config.validation_max_flow:
                return False, f"Flow above maximum ({reading.value:.2f} > {self.config.validation_max_flow})"

        # Check if device flagged as corrupted
        if reading.is_corrupted:
            return False, "Device flagged reading as corrupted"

        return True, None

    def _apply_moving_average(
        self,
        node_id: str,
        reading_type: str,
        value: float
    ) -> float:
        """
        Apply moving average filter.

        Returns:
            Filtered value
        """
        # Ensure window exists
        if node_id not in self._filter_windows:
            self._filter_windows[node_id] = {}
        if reading_type not in self._filter_windows[node_id]:
            self._filter_windows[node_id][reading_type] = deque(
                maxlen=self.config.moving_average_window
            )

        window = self._filter_windows[node_id][reading_type]
        window.append(value)

        # Return average
        return sum(window) / len(window)

    def _calculate_zscore(
        self,
        node_id: str,
        reading_type: str,
        value: float
    ) -> Optional[float]:
        """
        Calculate Z-score based on FROZEN baseline statistics.
        
        Uses a locked baseline from system initialization rather than
        a rolling window, to prevent leaks from corrupting the baseline.

        Returns:
            Z-score or None if insufficient history
        """
        key = (node_id, reading_type)
        
        # Check if we have a frozen baseline
        if self._baseline_locked and key in self._baseline_snapshot:
            baseline = self._baseline_snapshot[key]
            if baseline['std'] < 1e-6:
                return 0.0
            return (value - baseline['mean']) / baseline['std']
        
        # Still building baseline - use rolling window
        stats = self._buffer.get_windowed_statistics(
            node_id,
            reading_type,
            self.config.zscore_window
        )

        if stats is None or stats['count'] < self._baseline_samples_needed:
            return None
        
        # Save baseline snapshot if not locked yet
        if not self._baseline_locked and stats['count'] >= self._baseline_samples_needed:
            if key not in self._baseline_snapshot:
                self._baseline_snapshot[key] = {
                    'mean': stats['mean'],
                    'std': max(stats['std'], 0.1)  # Minimum std to avoid division issues
                }
                logger.debug(f"Baseline captured for {node_id}/{reading_type}: mean={stats['mean']:.2f}, std={stats['std']:.2f}")

        if stats['std'] < 1e-6:
            return 0.0

        z_score = (value - stats['mean']) / stats['std']
        return z_score

    def process(self, reading: SensorReading) -> Optional[TelemetryRecord]:
        """
        Process a sensor reading through the full pipeline.

        Pipeline: Validate -> Store -> Filter -> Normalize

        Args:
            reading: Raw sensor reading

        Returns:
            Processed TelemetryRecord or None if invalid
        """
        self._processed_count += 1

        # Stage 1: Validation
        is_valid, error_msg = self._validate(reading)

        record = TelemetryRecord(
            node_id=reading.node_id,
            timestamp=reading.timestamp,
            reading_type=reading.reading_type,
            raw_value=reading.value if is_valid else None,
            is_valid=is_valid,
            validation_error=error_msg
        )

        if not is_valid:
            self._invalid_count += 1
            self._emit_event(
                'validation_error',
                reading.node_id,
                reading.timestamp,
                f"Invalid data discarded: {error_msg}",
                'warning'
            )
            # Still store the record for tracking purposes
            self._buffer.store(record)
            return None

        self._valid_count += 1

        # Stage 2: Store raw value
        # (We'll update the record with filtered/normalized values)

        # Stage 3: Apply moving average filter
        filtered_value = self._apply_moving_average(
            reading.node_id,
            reading.reading_type,
            reading.value
        )
        record.filtered_value = filtered_value

        # Stage 4: Calculate Z-score normalization
        # IMPORTANT: When baseline is locked (leak detection mode), use RAW value
        # for z-score calculation. The moving average lags too much to detect
        # sudden changes. When not locked (building baseline), use filtered value.
        value_for_zscore = reading.value if self._baseline_locked else filtered_value
        z_score = self._calculate_zscore(
            reading.node_id,
            reading.reading_type,
            value_for_zscore
        )
        record.z_score = z_score

        # Store the complete record
        self._buffer.store(record)

        self._emit_event(
            'data_processed',
            reading.node_id,
            reading.timestamp,
            f"{reading.reading_type}: raw={reading.value:.2f}, filtered={filtered_value:.2f}, z={z_score:.2f}" if z_score else f"{reading.reading_type}: raw={reading.value:.2f}, filtered={filtered_value:.2f}",
            'info'
        )

        return record

    def process_batch(
        self,
        readings: List[SensorReading]
    ) -> List[TelemetryRecord]:
        """
        Process multiple readings.

        Args:
            readings: List of sensor readings

        Returns:
            List of valid processed records
        """
        results = []
        for reading in readings:
            record = self.process(reading)
            if record is not None:
                results.append(record)
        return results

    def get_latest_record(
        self,
        node_id: str,
        reading_type: str
    ) -> Optional[TelemetryRecord]:
        """Get the most recent valid record for a node."""
        records = self._buffer.get_recent(node_id, reading_type, 1)
        return records[0] if records else None

    def get_node_statistics(
        self,
        node_id: str,
        reading_type: str
    ) -> Optional[Dict]:
        """Get statistics for a node."""
        return self._buffer.get_windowed_statistics(node_id, reading_type)

    def get_all_latest_zscores(self) -> Dict[str, Dict[str, float]]:
        """
        Get latest Z-scores for all nodes.

        Returns:
            Dict[node_id, Dict[reading_type, z_score]]
        """
        result = {}
        for node_id in self._buffer.get_all_nodes():
            result[node_id] = {}
            for reading_type in self._buffer.get_node_types(node_id):
                records = self._buffer.get_recent(node_id, reading_type, 1)
                if records and records[0].z_score is not None:
                    result[node_id][reading_type] = records[0].z_score
        return result

    @property
    def buffer(self) -> TelemetryBuffer:
        """Access to the underlying buffer."""
        return self._buffer

    @property
    def statistics(self) -> Dict:
        """Get pipeline processing statistics."""
        return {
            'processed': self._processed_count,
            'valid': self._valid_count,
            'invalid': self._invalid_count,
            'validity_rate': (
                self._valid_count / self._processed_count
                if self._processed_count > 0 else 0.0
            )
        }

    def lock_baseline(self):
        """Lock the baseline - Z-scores will now be calculated against frozen baseline."""
        if self._baseline_snapshot:
            self._baseline_locked = True
            logger.info(f"Baseline locked with {len(self._baseline_snapshot)} sensor baselines")
        else:
            logger.warning("Cannot lock baseline - no baseline data captured yet")
    
    def unlock_baseline(self):
        """Unlock baseline to allow re-adaptation."""
        self._baseline_locked = False
        logger.info("Baseline unlocked")
    
    def clear_baseline(self):
        """Clear the baseline snapshot."""
        self._baseline_snapshot.clear()
        self._baseline_locked = False
        logger.info("Baseline cleared")

    def reset(self):
        """Reset the pipeline state."""
        self._buffer.clear()
        self._filter_windows.clear()
        self._processed_count = 0
        self._valid_count = 0
        self._invalid_count = 0
        self._baseline_snapshot.clear()
        self._baseline_locked = False
